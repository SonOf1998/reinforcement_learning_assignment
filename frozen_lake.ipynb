{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake8x8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**S**: Starting position  \n",
    "**F**: Frozen platform  \n",
    "**H**: Hole  \n",
    "**G**: Goal  \n",
    "  \n",
    "actions: [**LEFT**, **DOWN**, **RIGHT**, **UP**]\n",
    "  \n",
    "The goal during this game is to reach the tile 'G' starting from the tile 'S' without falling into a hole ('H').  \n",
    "The environment is -by default- slippery. This means that we cannot guarantee that a particular action will move the agent to the direction it says it will. The environment doesn't allow the agent to go off the map so a step that would cause exiting will have no effect.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "PROB = 0\n",
    "S_PRIME = 1\n",
    "REWARD = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State transition nondeterminism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By playing a quick game choosing random actions exclusively and printing the info variable that the environment returns after a taken step we can now see that we're only having 1/3 chance of going to the direction we selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "{'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "{'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "{'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=True) # is_slippery by default True\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(0, 3):\n",
    "    random_action = env.action_space.sample()        #choose a random action\n",
    "    new_s, r, done, info = env.step(random_action)\n",
    "    print(info)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further analize each step by looking into the *env.P[state][action]* data structure which will reveal how the state transitions work.    \n",
    "Now, we can clearly see that if we choose a specific direction we have only 1/3 chance that the agent will follow our command the way we would like. The agent might go to 90 degrees left or 90 degrees right to the preferred direction (with probability of 1/3 each) and never the opposite way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3333333333333333, 10, 0.0, False)\n",
      "(0.3333333333333333, 1, 0.0, False)\n",
      "(0.3333333333333333, 8, 0.0, False)\n"
     ]
    }
   ],
   "source": [
    "env.P[9][UP]\n",
    "for info in env.P[9][UP]:\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_state_action_value(env, state_values, s, a, dsc_rate):\n",
    "    q = 0\n",
    "    for info in env.P[s][a]:\n",
    "        q += info[PROB]*(info[REWARD] + dsc_rate * state_values[info[S_PRIME]])\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, theta, dsc_rate):\n",
    "    state_values = np.zeros(64)\n",
    "    delta = np.inf\n",
    "    \n",
    "    while delta >= theta:\n",
    "        delta = 0\n",
    "        \n",
    "        for s in range(64):\n",
    "            v = state_values[s]\n",
    "            new_v = 0\n",
    "            for a in range(4):\n",
    "                new_v += policy[s][a] * expected_state_action_value(env, state_values, s, a, dsc_rate)\n",
    "                \n",
    "            state_values[s] = new_v\n",
    "            delta = max(delta, np.abs(v - new_v))\n",
    "    \n",
    "    return state_values             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy, state_values, dsc_rate):\n",
    "    new_policy = np.zeros((64, 4))  \n",
    "    q = np.zeros((64, 4))           # state-action values\n",
    "    \n",
    "    for s in range(64):\n",
    "        for a in range(4):\n",
    "            q[s][a] = expected_state_action_value(env, state_values, s, a, dsc_rate)\n",
    "        a_index = q[s].argmax()       \n",
    "        new_policy[s] = np.zeros(4)\n",
    "        new_policy[s][a_index] = 1\n",
    "           \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, theta=0.00001, dsc_rate=0.99):\n",
    "    env.reset()\n",
    "    state_values = np.zeros(64)\n",
    "    policy = np.array([[0.25, 0.25, 0.25, 0.25],] * 64)\n",
    "    \n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        state_values = policy_evaluation(env, policy, theta, dsc_rate)\n",
    "        new_policy = policy_improvement(env, policy, state_values, dsc_rate)\n",
    "        if (policy == new_policy).all():\n",
    "            policy_stable = True\n",
    "            \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488 ms ± 36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "policy = policy_iteration(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_frozen_lake(env, policy, amount_of_games = 1):\n",
    "    won_games = 0\n",
    "    for _ in range(amount_of_games):\n",
    "        env.reset()\n",
    "        s = 0\n",
    "        while True:\n",
    "            action = policy[s].argmax()\n",
    "            new_s, r, done, info = env.step(action)\n",
    "            if done:\n",
    "                if r == 1.0:\n",
    "                    # we successfully reached our goal\n",
    "                    won_games += 1\n",
    "                break\n",
    "            s = new_s            \n",
    "    \n",
    "    win_ratio = won_games / amount_of_games\n",
    "    print(\"We won {:.2%} of the games\".format(win_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We won 86.00% of the games\n"
     ]
    }
   ],
   "source": [
    "play_frozen_lake(env, policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the policy iteration method converges to the (near) optimal policy in 488ms, averagely.\n",
    "Following the policy we computed we win the frozen lake game in 86% of the times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.00001, dsc_rate=0.99):\n",
    "    env.reset()\n",
    "    state_values = np.zeros(64)\n",
    "    policy = np.array([[0.25, 0.25, 0.25, 0.25],] * 64)\n",
    "    delta = np.inf\n",
    "    \n",
    "    while delta >= theta:\n",
    "        delta = 0\n",
    "        \n",
    "        for s in range(64):\n",
    "            v = state_values[s]\n",
    "            candidate_v = np.zeros(4)\n",
    "                       \n",
    "            for a in range(4):\n",
    "                candidate_v[a] = expected_state_action_value(env, state_values, s, a, dsc_rate)\n",
    "            \n",
    "            new_v = candidate_v.max()\n",
    "            delta = max(delta, np.abs(v - new_v))            \n",
    "            state_values[s] = candidate_v.max()\n",
    "    \n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 ms ± 6.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "policy = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We won 87.40% of the games\n"
     ]
    }
   ],
   "source": [
    "play_frozen_lake(env, policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration seemingly performed overall better:  \n",
    "The policy was found in shorter time and the win ratio became better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
